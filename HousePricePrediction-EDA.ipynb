{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dbf6883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your packages used to explore the data\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b01f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the data\n",
    "df = pd.read_csv(\"kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "07630231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the top 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0bc331b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can already see we do not need some of these columns. \n",
    "#I would initially already remove the date and id columns as they will have no impact on the predicting factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6151d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, before we do that lets explore some more.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0d94bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at data types.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8c8cf6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great. We can already see what we need to amend for this model. Price, bedrooms, bathrooms and floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2db43e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, before we start cleaning our data. Lets look at the correlation. \n",
    "#There is no point wasting time amending data that we will later delete\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bdc5d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make the above a bit more legible\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(df.corr(), cmap=\"BuGn\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "91084a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets change price to a integer so we have a whole number to predict.\n",
    "df = df.astype({'price': np.int64})\n",
    "# In this example I have made bathrooms an integer. \n",
    "#However, it would be prudent to first round the number down, and then make it an integer\n",
    "df = df.astype({'bathrooms': np.int64})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6de031b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we had any alphanumeric characters, we could use the df_new = pd.get_dummies = ['column name'] to change to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f499ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create our split data. We remove our target variable (price) && any other weakly correlated variables\n",
    "# In our py file I selected the most probable search factors, as this would be presented to a end user to fill in\n",
    "# Many details such as conditions or grade, although impactful on price would not be as important in an initial valuation\n",
    "X = df.drop(['price', 'id', 'date', 'lat', 'long'], axis=1)\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ee6f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check that columns have correctly been dropped\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7e6addb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "04844aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets test the models, before we choose our best model for the .py files\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb316ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6603523350206355"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test the models we have before progressing into our .py files\n",
    "\n",
    "# First up is the linear regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ffb35181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5662796002447262"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second up is Decision Tree Regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dec_model = DecisionTreeRegressor()\n",
    "dec_model.fit(X_train, y_train)\n",
    "dec_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dc879310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821620708180751"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third up is the Random Forest Regression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_model = RandomForestRegressor()\n",
    "random_model.fit(X_train, y_train)\n",
    "random_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d946ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally is XGBoost Regression. I did not run this as I had not installed the package yet.\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did not run the xgboost as I did not have the package installed on my personal machine\n",
    "# Instead I went and used the Random Forest Regression for my project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
